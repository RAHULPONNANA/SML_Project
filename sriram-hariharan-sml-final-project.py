# -*- coding: utf-8 -*-
"""sml_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N9JvVQMHiBr3wA2uCa5tb4YrvuREVdsD
"""

# Commented out IPython magic to ensure Python compatibility.
#import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math
from __future__ import division
import sys
import nltk
import sklearn
# %matplotlib inline
np.random.seed(18) 
import warnings
warnings.filterwarnings('ignore')

import re
import spacy
import unicodedata
import re
from nltk.corpus import wordnet
from nltk.corpus import stopwords
import collections
from nltk.tokenize.toktok import ToktokTokenizer
from bs4 import BeautifulSoup
from nltk.stem.wordnet import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.linear_model import SGDClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import hamming_loss
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.linear_model import Perceptron
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score,mean_absolute_error,mean_absolute_error,r2_score,log_loss
from sklearn.tree import DecisionTreeRegressor

from sklearn.model_selection import cross_val_score,RandomizedSearchCV,GridSearchCV
 from sklearn.tree import DecisionTreeClassifier

#library to import files
from google.colab import files

from google.colab import drive
drive.mount('/content/drive')

data=pd.read_csv("/content/drive/MyDrive/Tidy_data.csv")

data = data.set_index('Unnamed: 0')
data = data.rename(index={'Unnamed: 0':'Id'})

new_data=pd.merge(data[(data['Q_Score']>5)] ,data[(data['A_Score']>=5)],how="inner")

new_data.shape

"""Cleaning Data

"""

plt.figure(figsize=(5, 5))
new_data.isnull().mean(axis=0).plot.barh()
plt.title("Ratio of missing values per columns")

print('Dupplicate entries: {}'.format(new_data.duplicated().sum()))
new_data.drop_duplicates(inplace = True)

new_data['Tag'] = new_data['Tag'].apply(lambda x: x.split())

new_data

all_tags = [item for sublist in new_data['Tag'].values for item in sublist]

len(all_tags)

my_set = set(all_tags)
unique_tags = list(my_set)
len(unique_tags)

flat_list = [item for sublist in new_data['Tag'].values for item in sublist]
keywords = nltk.FreqDist(flat_list)
keywords = nltk.FreqDist(keywords)
frequencies_words = keywords.most_common(100)
tags_features = [word[0] for word in frequencies_words]

len(keywords)

fig, ax = plt.subplots(figsize=(15, 15))
keywords.plot(100, cumulative=False)
fig.savefig("/content/drive/MyDrive/tag_distribution.jpg",bbox_inches='tight')

def most_common(tags):
  '''
  checking if tags in an entry is in the most common tags .
  '''
  tags_filtered = []
  for i in range(0, len(tags)):
      if tags[i] in tags_features:
          tags_filtered.append(tags[i])
  return tags_filtered

new_data['Tag'] = new_data['Tag'].apply(lambda x: most_common(x))
new_data['Tag'] = new_data['Tag'].apply(lambda x: x if len(x)>0 else None)

new_data.shape

new_data

new_data.dropna(subset=['Tag'], inplace=True)

new_data.shape

new_data.head()

"""Preprocessing Body


"""

def clean_text(text):
  '''
  removing escape characters and special characters
  '''
  text = text.lower()
  text = re.sub(r"what's", "what is ", text)
  text = re.sub(r"\'s", " ", text)
  text = re.sub(r"\'ve", " have ", text)
  text = re.sub(r"can't", "can not ", text)
  text = re.sub(r"won't", "will not ", text)
  text = re.sub(r"n't", " not ", text)
  text = re.sub(r"i'm", "i am ", text)
  text = re.sub(r"\'re", " are ", text)
  text = re.sub(r"\'d", " would ", text)
  text = re.sub(r"\'ll", " will ", text)
  text = re.sub(r"\'scuse", " excuse ", text)
  text = re.sub(r"\'\n", " ", text)
  text = re.sub(r"\'\xa0", " ", text)
  text = re.sub('\s+', ' ', text)
  text = text.strip(' ')
  return text

new_data['Title'] = new_data['Title'].apply(lambda x: clean_text(x))

new_data['Question'] = new_data['Question'].apply(lambda x: clean_text(x))

new_data['Answer'] = new_data['Answer'].apply(lambda x: clean_text(x))

new_data.head()

new_data['Answer']

token=ToktokTokenizer()

punct = '!"#$%&\'()*+,./:;<=>?@[\\]^_`{|}~'

def strip_list_noempty(mylist):
    newlist = (item.strip() if hasattr(item, 'strip') else item for item in mylist)
    return [item for item in newlist if item != '']

re.escape(punct)

re.compile('[%s]' % re.escape(punct))

def clean_punct(text): 
    words=token.tokenize(text)
    punctuation_filtered = []
    regex = re.compile('[%s]' % re.escape(punct))
    # remove_punctuation = str.maketrans(' ', ' ', punct)
    for w in words:
        if w in tags_features:
            punctuation_filtered.append(w)
        else:
            punctuation_filtered.append(regex.sub('', w))
  
    filtered_list = strip_list_noempty(punctuation_filtered)
        
    return ' '.join(map(str, filtered_list))

new_data['Answer'] = new_data['Answer'].apply(lambda x: clean_punct(x)) 
new_data['Question'] = new_data['Question'].apply(lambda x: clean_punct(x)) 
new_data['Title'] = new_data['Title'].apply(lambda x: clean_punct(x))

new_data['Answer'][45]

nltk.download('stopwords')

nltk.download('wordnet')

lemma=WordNetLemmatizer()
stop_words = set(stopwords.words("english"))

def lemitizeWords(text):
    words=token.tokenize(text)
    listLemma=[]
    for w in words:
        x=lemma.lemmatize(w, pos="v")
        listLemma.append(x)
    return ' '.join(map(str, listLemma))

def stopWordsRemove(text):
    stop_words = set(stopwords.words("english"))
    words=token.tokenize(text)
    filtered = [w for w in words if not w in stop_words]
    return ' '.join(map(str, filtered))

new_data['Answer'] = new_data['Answer'].apply(lambda x: lemitizeWords(x)) 
new_data['Answer'] = new_data['Answer'].apply(lambda x: stopWordsRemove(x))

new_data['Title'] = new_data['Title'].apply(lambda x: lemitizeWords(x)) 
new_data['Title'] = new_data['Title'].apply(lambda x: stopWordsRemove(x)) 

new_data['Question'] = new_data['Question'].apply(lambda x: lemitizeWords(x)) 
new_data['Question'] = new_data['Question'].apply(lambda x: stopWordsRemove(x)) 

new_data['Tag'] = new_data['Tag'].apply(lambda x: lemitizeWords(x)) 
new_data['Tag'] = new_data['Tag'].apply(lambda x: stopWordsRemove(x))

new_data

"""EDA

"""

no_topics = 20

text1= new_data['Title']
text2=new_data['Question']
text3=new_data['Answer']

vectorizer_train = TfidfVectorizer(analyzer = 'word',
                                       min_df=0.0,
                                       max_df = 1.0,
                                       strip_accents = None,
                                       encoding = 'utf-8', 
                                       preprocessor=None,
                                       token_pattern=r"(?u)\S\S+",
                                       max_features=1000)

"""Data preparation

"""

x1= new_data['Title']
x2= new_data['Question']
x3 = new_data['Answer']

new_data['Title']

y=new_data['Tag']

y

multilabel_binarizer = MultiLabelBinarizer()
y_bin = multilabel_binarizer.fit_transform(y)

vectorizer_X1 = TfidfVectorizer(analyzer = 'word',
                                       min_df=0.0,
                                       max_df = 1.0,strip_accents = None,encoding = 'utf-8', 
                                       preprocessor=None,token_pattern=r"(?u)\S\S+",
                                       max_features=1000)

vectorizer_X2 = TfidfVectorizer(analyzer = 'word',
                                       min_df=0.0,
                                       max_df = 1.0,strip_accents = None,encoding = 'utf-8', 
                                       preprocessor=None,token_pattern=r"(?u)\S\S+",
                                       max_features=1000)

vectorizer_X3 = TfidfVectorizer(analyzer = 'word',
                                       min_df=0.0,
                                       max_df = 1.0,strip_accents = None,encoding = 'utf-8', 
                                       preprocessor=None,token_pattern=r"(?u)\S\S+",
                                       max_features=1000)

X1_tfidf = vectorizer_X1.fit_transform(x1)
X2_tfidf = vectorizer_X2.fit_transform(x2)
X3_tfidf = vectorizer_X3.fit_transform(x3)

X_tfidf = np.hstack([X1_tfidf.A,X2_tfidf.A,X3_tfidf.A])

#X_tfidf = np.vstack([[X1_tfidf.A],[X2_tfidf.A],[X3_tfidf.A]])

X_tfidf.shape

y_bin.shape

multilabel_binarizer.classes_

X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_bin, test_size = 0.2, random_state = 0)

"""Models :

"""

def print_score(y_pred, clf):
    print("Clf: ", clf.__class__.__name__)
    print("Accuracy : {}".format(accuracy_score(y_pred,y_test)))
    print("Precision: {}".format(precision_score(y_pred,y_test,average="macro")))
    print("Recall value : {}".format(recall_score(y_pred,y_test,average="macro")))
    print("F1 score : {}".format(f1_score(y_pred,y_test,average="macro")))
    print("Log Loss : {}".format(log_loss(y_test,y_pred)))
    print("---")

lr = LogisticRegression(penalty="l2",max_iter=500)
clf = OneVsRestClassifier(lr)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
print_score(y_pred, lr)

sgd = SGDClassifier(penalty="l2",max_iter=500,alpha=0.0001)
clf = OneVsRestClassifier(sgd)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
print("Regularization parameter: {}, Maximum iterations: {}".format(0.001,500))
print("------------------------------>")
print_score(y_pred, sgd)

sgd = SGDClassifier(penalty="l2",max_iter=550,alpha=0.01)
clf = OneVsRestClassifier(sgd)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
print_score(y_pred, sgd)

"""Random Forest

"""

rfc = RandomForestClassifier()
rfc.fit(X_train, y_train)

y_pred = rfc.predict(X_test)
print("Cross validation score:",cross_val_score(rfc, X_train, y_train, cv=5).mean())
print_score(y_pred, rfc)

clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
print("Cross validation score:",cross_val_score(clf, X_train, y_train, cv=3).mean())
print_score(y_pred, clf)

regressor = DecisionTreeRegressor(random_state=0)
regressor.fit(X_train, y_train)

y_pred = regressor.predict(X_test)
print("Cross validation score:",cross_val_score(regressor, X_train, y_train, cv=3).mean())
print_score(y_pred, regressor)

param_grid = {
    "max_iter":[100,200,300],
    "alpha": [0.001,0.01,0.1],
    "n_estimators" : [300,400],
    "min_samples_split" : [2,3,4],
    "max_features" : [2000,3000]
}

"""Logistic Regression:"""

for max_iter in [10,20,50,80,100]:
      lr = LogisticRegression(penalty='l2',max_iter=max_iter,n_jobs=-1)
      clf = OneVsRestClassifier(lr)
      clf.fit(X_train, y_train)
      y_pred = clf.predict(X_test)
      print("Maximum iterations: {}".format(max_iter))
      print("------------------------------>")
      print_score(y_pred, lr)

for max_iter in param_grid['max_iter']:
  for alpha in param_grid['alpha']:
    sgd = SGDClassifier(penalty="l2",max_iter=max_iter,alpha=alpha)
    clf = OneVsRestClassifier(sgd)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print("Regularization parameter: {}, Maximum iterations: {}".format(alpha,max_iter))
    print("------------------------------>")
    print_score(y_pred, sgd)

for numF in param_grid['max_features']:
  for minSampSplit in param_grid['min_samples_split']:
    clf = DecisionTreeClassifier()
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print("Max features considered: {}, Minimum Samples threshold for split: {}".format(numF,minSampSplit))
    print("------------------------------>")
    # print("Cross validation score:",cross_val_score(clf, X_train, y_train, cv=5).mean())
    print_score(y_pred, clf)

for noEst in param_grid["n_estimators"]:
  for noMaxFeat in param_grid["max_features"]:
    for minSampSplit in param_grid['min_samples_split']:
      rfc = RandomForestClassifier(n_estimators=noEst,max_features=noMaxFeat,min_samples_split=minSampSplit)
      rfc.fit(X_train, y_train)
      y_pred = rfc.predict(X_test)
      print("No. of estimators: {}, Maximum features: {}, Minimum Samples threshold for split: {}".format(noEst,noMaxFeat,minSampSplit))
      print("------------------------------>")
      print_score(y_pred, rfc)
      # print("Cross validation score:",cross_val_score(rfc, X_train, y_train, cv=3).mean())